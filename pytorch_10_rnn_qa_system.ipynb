{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaileshps21/pytorch-/blob/main/pytorch_10_rnn_qa_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "e7gYEZsZlhhv"
      },
      "id": "e7gYEZsZlhhv",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('100_Unique_QA_Dataset.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kHhxCr66mb07",
        "outputId": "e90b58f6-e711-43d3-951b-65356a2fea2d"
      },
      "id": "kHhxCr66mb07",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             question        answer\n",
              "0                      What is the capital of France?         Paris\n",
              "1                     What is the capital of Germany?        Berlin\n",
              "2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
              "3     What is the largest planet in our solar system?       Jupiter\n",
              "4      What is the boiling point of water in Celsius?           100\n",
              "..                                                ...           ...\n",
              "85                  Who directed the movie 'Titanic'?  JamesCameron\n",
              "86  Which superhero is also known as the Dark Knight?        Batman\n",
              "87                     What is the capital of Brazil?      Brasilia\n",
              "88        Which fruit is known as the king of fruits?         Mango\n",
              "89       Which country is known for the Eiffel Tower?        France\n",
              "\n",
              "[90 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0253d28-600a-4956-98ae-870bd2427536\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the capital of Germany?</td>\n",
              "      <td>Berlin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
              "      <td>Harper-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest planet in our solar system?</td>\n",
              "      <td>Jupiter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the boiling point of water in Celsius?</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>Who directed the movie 'Titanic'?</td>\n",
              "      <td>JamesCameron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>Which superhero is also known as the Dark Knight?</td>\n",
              "      <td>Batman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>What is the capital of Brazil?</td>\n",
              "      <td>Brasilia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Which fruit is known as the king of fruits?</td>\n",
              "      <td>Mango</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Which country is known for the Eiffel Tower?</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0253d28-600a-4956-98ae-870bd2427536')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0253d28-600a-4956-98ae-870bd2427536 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0253d28-600a-4956-98ae-870bd2427536');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_ad5ca160-4765-4223-9e95-294c757983b7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ad5ca160-4765-4223-9e95-294c757983b7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90,\n        \"samples\": [\n          \"What is the currency of China?\",\n          \"What is the capital of Australia?\",\n          \"Who discovered electricity?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 85,\n        \"samples\": [\n          \"ChristopherColumbus\",\n          \"Paris\",\n          \"Christmas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "def tokenize(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace(\"?\" , \"\")\n",
        "  text = text.replace(\"'\", \"\")\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "fRvNPjI4mxD5"
      },
      "id": "fRvNPjI4mxD5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize('what is the capital of france?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwbOy1F1qEVI",
        "outputId": "aeecc3bc-d137-4f82-a797-47867d861493"
      },
      "id": "WwbOy1F1qEVI",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'is', 'the', 'capital', 'of', 'france']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary\n",
        "def build_vocab(row):\n",
        "    tokenized_question = tokenize(row['question'])\n",
        "    tokenized_answer   = tokenize(row['answer'])\n",
        "\n",
        "    merged_tokens = tokenized_question + tokenized_answer\n",
        "    print(merged_tokens)\n",
        "\n",
        "    for token in merged_tokens:\n",
        "      if token not in vocab:\n",
        "        vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "fSsl3kHgo8og"
      },
      "id": "fSsl3kHgo8og",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<UNK>': 0}\n",
        "df.apply(build_vocab , axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "K7e1lUfdpm6S",
        "outputId": "15d68ee1-ceb2-4339-fa75-5ae4d80acfbf"
      },
      "id": "K7e1lUfdpm6S",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
            "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
            "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
            "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
            "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
            "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
            "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
            "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
            "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
            "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
            "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
            "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
            "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
            "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
            "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
            "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
            "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
            "['who', 'discovered', 'gravity', 'newton']\n",
            "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
            "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
            "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
            "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
            "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
            "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
            "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
            "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
            "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
            "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
            "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
            "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
            "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
            "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
            "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
            "['who', 'painted', 'starry', 'night', 'vangogh']\n",
            "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
            "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
            "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
            "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
            "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
            "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
            "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
            "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
            "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
            "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
            "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
            "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
            "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
            "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
            "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
            "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
            "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
            "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
            "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
            "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
            "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
            "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
            "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
            "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
            "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
            "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
            "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
            "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
            "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
            "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
            "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
            "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
            "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
            "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
            "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
            "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
            "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
            "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
            "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
            "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
            "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
            "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
            "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
            "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
            "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
            "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
            "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
            "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
            "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
            "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
            "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
            "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
            "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
            "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
            "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
            "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     None\n",
              "1     None\n",
              "2     None\n",
              "3     None\n",
              "4     None\n",
              "      ... \n",
              "85    None\n",
              "86    None\n",
              "87    None\n",
              "88    None\n",
              "89    None\n",
              "Length: 90, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iTcB2Wf7psEc",
        "outputId": "7e99e468-1919-49ae-f3a3-649a0f1d2cd9"
      },
      "id": "iTcB2Wf7psEc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " 'paris': 7,\n",
              " 'germany': 8,\n",
              " 'berlin': 9,\n",
              " 'who': 10,\n",
              " 'wrote': 11,\n",
              " 'to': 12,\n",
              " 'kill': 13,\n",
              " 'a': 14,\n",
              " 'mockingbird': 15,\n",
              " 'harper-lee': 16,\n",
              " 'largest': 17,\n",
              " 'planet': 18,\n",
              " 'in': 19,\n",
              " 'our': 20,\n",
              " 'solar': 21,\n",
              " 'system': 22,\n",
              " 'jupiter': 23,\n",
              " 'boiling': 24,\n",
              " 'point': 25,\n",
              " 'water': 26,\n",
              " 'celsius': 27,\n",
              " '100': 28,\n",
              " 'painted': 29,\n",
              " 'mona': 30,\n",
              " 'lisa': 31,\n",
              " 'leonardo-da-vinci': 32,\n",
              " 'square': 33,\n",
              " 'root': 34,\n",
              " '64': 35,\n",
              " '8': 36,\n",
              " 'chemical': 37,\n",
              " 'symbol': 38,\n",
              " 'for': 39,\n",
              " 'gold': 40,\n",
              " 'au': 41,\n",
              " 'which': 42,\n",
              " 'year': 43,\n",
              " 'did': 44,\n",
              " 'world': 45,\n",
              " 'war': 46,\n",
              " 'ii': 47,\n",
              " 'end': 48,\n",
              " '1945': 49,\n",
              " 'longest': 50,\n",
              " 'river': 51,\n",
              " 'nile': 52,\n",
              " 'japan': 53,\n",
              " 'tokyo': 54,\n",
              " 'developed': 55,\n",
              " 'theory': 56,\n",
              " 'relativity': 57,\n",
              " 'albert-einstein': 58,\n",
              " 'freezing': 59,\n",
              " 'fahrenheit': 60,\n",
              " '32': 61,\n",
              " 'known': 62,\n",
              " 'as': 63,\n",
              " 'red': 64,\n",
              " 'mars': 65,\n",
              " 'author': 66,\n",
              " '1984': 67,\n",
              " 'george-orwell': 68,\n",
              " 'currency': 69,\n",
              " 'united': 70,\n",
              " 'kingdom': 71,\n",
              " 'pound': 72,\n",
              " 'india': 73,\n",
              " 'delhi': 74,\n",
              " 'discovered': 75,\n",
              " 'gravity': 76,\n",
              " 'newton': 77,\n",
              " 'how': 78,\n",
              " 'many': 79,\n",
              " 'continents': 80,\n",
              " 'are': 81,\n",
              " 'there': 82,\n",
              " 'on': 83,\n",
              " 'earth': 84,\n",
              " '7': 85,\n",
              " 'gas': 86,\n",
              " 'do': 87,\n",
              " 'plants': 88,\n",
              " 'use': 89,\n",
              " 'photosynthesis': 90,\n",
              " 'co2': 91,\n",
              " 'smallest': 92,\n",
              " 'prime': 93,\n",
              " 'number': 94,\n",
              " '2': 95,\n",
              " 'invented': 96,\n",
              " 'telephone': 97,\n",
              " 'alexander-graham-bell': 98,\n",
              " 'australia': 99,\n",
              " 'canberra': 100,\n",
              " 'ocean': 101,\n",
              " 'pacific-ocean': 102,\n",
              " 'speed': 103,\n",
              " 'light': 104,\n",
              " 'vacuum': 105,\n",
              " '299,792,458m/s': 106,\n",
              " 'language': 107,\n",
              " 'spoken': 108,\n",
              " 'brazil': 109,\n",
              " 'portuguese': 110,\n",
              " 'penicillin': 111,\n",
              " 'alexander-fleming': 112,\n",
              " 'canada': 113,\n",
              " 'ottawa': 114,\n",
              " 'mammal': 115,\n",
              " 'whale': 116,\n",
              " 'element': 117,\n",
              " 'has': 118,\n",
              " 'atomic': 119,\n",
              " '1': 120,\n",
              " 'hydrogen': 121,\n",
              " 'tallest': 122,\n",
              " 'mountain': 123,\n",
              " 'everest': 124,\n",
              " 'city': 125,\n",
              " 'big': 126,\n",
              " 'apple': 127,\n",
              " 'newyork': 128,\n",
              " 'planets': 129,\n",
              " 'starry': 130,\n",
              " 'night': 131,\n",
              " 'vangogh': 132,\n",
              " 'formula': 133,\n",
              " 'h2o': 134,\n",
              " 'italy': 135,\n",
              " 'rome': 136,\n",
              " 'country': 137,\n",
              " 'famous': 138,\n",
              " 'sushi': 139,\n",
              " 'was': 140,\n",
              " 'first': 141,\n",
              " 'person': 142,\n",
              " 'step': 143,\n",
              " 'moon': 144,\n",
              " 'armstrong': 145,\n",
              " 'main': 146,\n",
              " 'ingredient': 147,\n",
              " 'guacamole': 148,\n",
              " 'avocado': 149,\n",
              " 'sides': 150,\n",
              " 'does': 151,\n",
              " 'hexagon': 152,\n",
              " 'have': 153,\n",
              " '6': 154,\n",
              " 'china': 155,\n",
              " 'yuan': 156,\n",
              " 'pride': 157,\n",
              " 'and': 158,\n",
              " 'prejudice': 159,\n",
              " 'jane-austen': 160,\n",
              " 'iron': 161,\n",
              " 'fe': 162,\n",
              " 'hardest': 163,\n",
              " 'natural': 164,\n",
              " 'substance': 165,\n",
              " 'diamond': 166,\n",
              " 'continent': 167,\n",
              " 'by': 168,\n",
              " 'area': 169,\n",
              " 'asia': 170,\n",
              " 'president': 171,\n",
              " 'states': 172,\n",
              " 'george-washington': 173,\n",
              " 'bird': 174,\n",
              " 'its': 175,\n",
              " 'ability': 176,\n",
              " 'mimic': 177,\n",
              " 'sounds': 178,\n",
              " 'parrot': 179,\n",
              " 'longest-running': 180,\n",
              " 'animated': 181,\n",
              " 'tv': 182,\n",
              " 'show': 183,\n",
              " 'simpsons': 184,\n",
              " 'vaticancity': 185,\n",
              " 'most': 186,\n",
              " 'moons': 187,\n",
              " 'saturn': 188,\n",
              " 'romeo': 189,\n",
              " 'juliet': 190,\n",
              " 'shakespeare': 191,\n",
              " 'earths': 192,\n",
              " 'atmosphere': 193,\n",
              " 'nitrogen': 194,\n",
              " 'bones': 195,\n",
              " 'adult': 196,\n",
              " 'human': 197,\n",
              " 'body': 198,\n",
              " '206': 199,\n",
              " 'metal': 200,\n",
              " 'liquid': 201,\n",
              " 'at': 202,\n",
              " 'room': 203,\n",
              " 'temperature': 204,\n",
              " 'mercury': 205,\n",
              " 'russia': 206,\n",
              " 'moscow': 207,\n",
              " 'electricity': 208,\n",
              " 'benjamin-franklin': 209,\n",
              " 'second-largest': 210,\n",
              " 'land': 211,\n",
              " 'color': 212,\n",
              " 'ripe': 213,\n",
              " 'banana': 214,\n",
              " 'yellow': 215,\n",
              " 'month': 216,\n",
              " '28': 217,\n",
              " 'days': 218,\n",
              " 'common': 219,\n",
              " 'february': 220,\n",
              " 'study': 221,\n",
              " 'living': 222,\n",
              " 'organisms': 223,\n",
              " 'called': 224,\n",
              " 'biology': 225,\n",
              " 'home': 226,\n",
              " 'great': 227,\n",
              " 'wall': 228,\n",
              " 'bees': 229,\n",
              " 'collect': 230,\n",
              " 'from': 231,\n",
              " 'flowers': 232,\n",
              " 'nectar': 233,\n",
              " 'opposite': 234,\n",
              " 'day': 235,\n",
              " 'south': 236,\n",
              " 'korea': 237,\n",
              " 'seoul': 238,\n",
              " 'bulb': 239,\n",
              " 'edison': 240,\n",
              " 'humans': 241,\n",
              " 'breathe': 242,\n",
              " 'survival': 243,\n",
              " 'oxygen': 244,\n",
              " '144': 245,\n",
              " '12': 246,\n",
              " 'pyramids': 247,\n",
              " 'giza': 248,\n",
              " 'egypt': 249,\n",
              " 'sea': 250,\n",
              " 'creature': 251,\n",
              " 'eight': 252,\n",
              " 'arms': 253,\n",
              " 'octopus': 254,\n",
              " 'holiday': 255,\n",
              " 'celebrated': 256,\n",
              " 'december': 257,\n",
              " '25': 258,\n",
              " 'christmas': 259,\n",
              " 'yen': 260,\n",
              " 'legs': 261,\n",
              " 'spider': 262,\n",
              " 'sport': 263,\n",
              " 'uses': 264,\n",
              " 'net,': 265,\n",
              " 'ball,': 266,\n",
              " 'hoop': 267,\n",
              " 'basketball': 268,\n",
              " 'kangaroos': 269,\n",
              " 'female': 270,\n",
              " 'minister': 271,\n",
              " 'uk': 272,\n",
              " 'margaretthatcher': 273,\n",
              " 'fastest': 274,\n",
              " 'animal': 275,\n",
              " 'cheetah': 276,\n",
              " 'periodic': 277,\n",
              " 'table': 278,\n",
              " 'spain': 279,\n",
              " 'madrid': 280,\n",
              " 'closest': 281,\n",
              " 'sun': 282,\n",
              " 'father': 283,\n",
              " 'computers': 284,\n",
              " 'charlesbabbage': 285,\n",
              " 'mexico': 286,\n",
              " 'mexicocity': 287,\n",
              " 'colors': 288,\n",
              " 'rainbow': 289,\n",
              " 'musical': 290,\n",
              " 'instrument': 291,\n",
              " 'black': 292,\n",
              " 'white': 293,\n",
              " 'keys': 294,\n",
              " 'piano': 295,\n",
              " 'americas': 296,\n",
              " '1492': 297,\n",
              " 'christophercolumbus': 298,\n",
              " 'disney': 299,\n",
              " 'character': 300,\n",
              " 'long': 301,\n",
              " 'nose': 302,\n",
              " 'grows': 303,\n",
              " 'it': 304,\n",
              " 'when': 305,\n",
              " 'lying': 306,\n",
              " 'pinocchio': 307,\n",
              " 'directed': 308,\n",
              " 'movie': 309,\n",
              " 'titanic': 310,\n",
              " 'jamescameron': 311,\n",
              " 'superhero': 312,\n",
              " 'also': 313,\n",
              " 'dark': 314,\n",
              " 'knight': 315,\n",
              " 'batman': 316,\n",
              " 'brasilia': 317,\n",
              " 'fruit': 318,\n",
              " 'king': 319,\n",
              " 'fruits': 320,\n",
              " 'mango': 321,\n",
              " 'eiffel': 322,\n",
              " 'tower': 323}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert word to numerical indices -- why i am doing it?\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "  indexed_text = []\n",
        "  for token in tokenize(text):\n",
        "    if token in vocab:\n",
        "      indexed_text.append(vocab[token])\n",
        "    else:\n",
        "      indexed_text.append(vocab['<UNK>'])\n",
        "\n",
        "  return indexed_text"
      ],
      "metadata": {
        "id": "L6xiNXb5tBE2"
      },
      "id": "L6xiNXb5tBE2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_indices(\"What is France\" , vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-7QRQHpx9U4",
        "outputId": "a055a135-0d14-4cb8-eec3-950aaee3d906"
      },
      "id": "f-7QRQHpx9U4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9URTVjV8yMlQ",
        "outputId": "acad7de4-d82c-4874-a6ca-5ef7e1cd3110"
      },
      "id": "9URTVjV8yMlQ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " 'paris': 7,\n",
              " 'germany': 8,\n",
              " 'berlin': 9,\n",
              " 'who': 10,\n",
              " 'wrote': 11,\n",
              " 'to': 12,\n",
              " 'kill': 13,\n",
              " 'a': 14,\n",
              " 'mockingbird': 15,\n",
              " 'harper-lee': 16,\n",
              " 'largest': 17,\n",
              " 'planet': 18,\n",
              " 'in': 19,\n",
              " 'our': 20,\n",
              " 'solar': 21,\n",
              " 'system': 22,\n",
              " 'jupiter': 23,\n",
              " 'boiling': 24,\n",
              " 'point': 25,\n",
              " 'water': 26,\n",
              " 'celsius': 27,\n",
              " '100': 28,\n",
              " 'painted': 29,\n",
              " 'mona': 30,\n",
              " 'lisa': 31,\n",
              " 'leonardo-da-vinci': 32,\n",
              " 'square': 33,\n",
              " 'root': 34,\n",
              " '64': 35,\n",
              " '8': 36,\n",
              " 'chemical': 37,\n",
              " 'symbol': 38,\n",
              " 'for': 39,\n",
              " 'gold': 40,\n",
              " 'au': 41,\n",
              " 'which': 42,\n",
              " 'year': 43,\n",
              " 'did': 44,\n",
              " 'world': 45,\n",
              " 'war': 46,\n",
              " 'ii': 47,\n",
              " 'end': 48,\n",
              " '1945': 49,\n",
              " 'longest': 50,\n",
              " 'river': 51,\n",
              " 'nile': 52,\n",
              " 'japan': 53,\n",
              " 'tokyo': 54,\n",
              " 'developed': 55,\n",
              " 'theory': 56,\n",
              " 'relativity': 57,\n",
              " 'albert-einstein': 58,\n",
              " 'freezing': 59,\n",
              " 'fahrenheit': 60,\n",
              " '32': 61,\n",
              " 'known': 62,\n",
              " 'as': 63,\n",
              " 'red': 64,\n",
              " 'mars': 65,\n",
              " 'author': 66,\n",
              " '1984': 67,\n",
              " 'george-orwell': 68,\n",
              " 'currency': 69,\n",
              " 'united': 70,\n",
              " 'kingdom': 71,\n",
              " 'pound': 72,\n",
              " 'india': 73,\n",
              " 'delhi': 74,\n",
              " 'discovered': 75,\n",
              " 'gravity': 76,\n",
              " 'newton': 77,\n",
              " 'how': 78,\n",
              " 'many': 79,\n",
              " 'continents': 80,\n",
              " 'are': 81,\n",
              " 'there': 82,\n",
              " 'on': 83,\n",
              " 'earth': 84,\n",
              " '7': 85,\n",
              " 'gas': 86,\n",
              " 'do': 87,\n",
              " 'plants': 88,\n",
              " 'use': 89,\n",
              " 'photosynthesis': 90,\n",
              " 'co2': 91,\n",
              " 'smallest': 92,\n",
              " 'prime': 93,\n",
              " 'number': 94,\n",
              " '2': 95,\n",
              " 'invented': 96,\n",
              " 'telephone': 97,\n",
              " 'alexander-graham-bell': 98,\n",
              " 'australia': 99,\n",
              " 'canberra': 100,\n",
              " 'ocean': 101,\n",
              " 'pacific-ocean': 102,\n",
              " 'speed': 103,\n",
              " 'light': 104,\n",
              " 'vacuum': 105,\n",
              " '299,792,458m/s': 106,\n",
              " 'language': 107,\n",
              " 'spoken': 108,\n",
              " 'brazil': 109,\n",
              " 'portuguese': 110,\n",
              " 'penicillin': 111,\n",
              " 'alexander-fleming': 112,\n",
              " 'canada': 113,\n",
              " 'ottawa': 114,\n",
              " 'mammal': 115,\n",
              " 'whale': 116,\n",
              " 'element': 117,\n",
              " 'has': 118,\n",
              " 'atomic': 119,\n",
              " '1': 120,\n",
              " 'hydrogen': 121,\n",
              " 'tallest': 122,\n",
              " 'mountain': 123,\n",
              " 'everest': 124,\n",
              " 'city': 125,\n",
              " 'big': 126,\n",
              " 'apple': 127,\n",
              " 'newyork': 128,\n",
              " 'planets': 129,\n",
              " 'starry': 130,\n",
              " 'night': 131,\n",
              " 'vangogh': 132,\n",
              " 'formula': 133,\n",
              " 'h2o': 134,\n",
              " 'italy': 135,\n",
              " 'rome': 136,\n",
              " 'country': 137,\n",
              " 'famous': 138,\n",
              " 'sushi': 139,\n",
              " 'was': 140,\n",
              " 'first': 141,\n",
              " 'person': 142,\n",
              " 'step': 143,\n",
              " 'moon': 144,\n",
              " 'armstrong': 145,\n",
              " 'main': 146,\n",
              " 'ingredient': 147,\n",
              " 'guacamole': 148,\n",
              " 'avocado': 149,\n",
              " 'sides': 150,\n",
              " 'does': 151,\n",
              " 'hexagon': 152,\n",
              " 'have': 153,\n",
              " '6': 154,\n",
              " 'china': 155,\n",
              " 'yuan': 156,\n",
              " 'pride': 157,\n",
              " 'and': 158,\n",
              " 'prejudice': 159,\n",
              " 'jane-austen': 160,\n",
              " 'iron': 161,\n",
              " 'fe': 162,\n",
              " 'hardest': 163,\n",
              " 'natural': 164,\n",
              " 'substance': 165,\n",
              " 'diamond': 166,\n",
              " 'continent': 167,\n",
              " 'by': 168,\n",
              " 'area': 169,\n",
              " 'asia': 170,\n",
              " 'president': 171,\n",
              " 'states': 172,\n",
              " 'george-washington': 173,\n",
              " 'bird': 174,\n",
              " 'its': 175,\n",
              " 'ability': 176,\n",
              " 'mimic': 177,\n",
              " 'sounds': 178,\n",
              " 'parrot': 179,\n",
              " 'longest-running': 180,\n",
              " 'animated': 181,\n",
              " 'tv': 182,\n",
              " 'show': 183,\n",
              " 'simpsons': 184,\n",
              " 'vaticancity': 185,\n",
              " 'most': 186,\n",
              " 'moons': 187,\n",
              " 'saturn': 188,\n",
              " 'romeo': 189,\n",
              " 'juliet': 190,\n",
              " 'shakespeare': 191,\n",
              " 'earths': 192,\n",
              " 'atmosphere': 193,\n",
              " 'nitrogen': 194,\n",
              " 'bones': 195,\n",
              " 'adult': 196,\n",
              " 'human': 197,\n",
              " 'body': 198,\n",
              " '206': 199,\n",
              " 'metal': 200,\n",
              " 'liquid': 201,\n",
              " 'at': 202,\n",
              " 'room': 203,\n",
              " 'temperature': 204,\n",
              " 'mercury': 205,\n",
              " 'russia': 206,\n",
              " 'moscow': 207,\n",
              " 'electricity': 208,\n",
              " 'benjamin-franklin': 209,\n",
              " 'second-largest': 210,\n",
              " 'land': 211,\n",
              " 'color': 212,\n",
              " 'ripe': 213,\n",
              " 'banana': 214,\n",
              " 'yellow': 215,\n",
              " 'month': 216,\n",
              " '28': 217,\n",
              " 'days': 218,\n",
              " 'common': 219,\n",
              " 'february': 220,\n",
              " 'study': 221,\n",
              " 'living': 222,\n",
              " 'organisms': 223,\n",
              " 'called': 224,\n",
              " 'biology': 225,\n",
              " 'home': 226,\n",
              " 'great': 227,\n",
              " 'wall': 228,\n",
              " 'bees': 229,\n",
              " 'collect': 230,\n",
              " 'from': 231,\n",
              " 'flowers': 232,\n",
              " 'nectar': 233,\n",
              " 'opposite': 234,\n",
              " 'day': 235,\n",
              " 'south': 236,\n",
              " 'korea': 237,\n",
              " 'seoul': 238,\n",
              " 'bulb': 239,\n",
              " 'edison': 240,\n",
              " 'humans': 241,\n",
              " 'breathe': 242,\n",
              " 'survival': 243,\n",
              " 'oxygen': 244,\n",
              " '144': 245,\n",
              " '12': 246,\n",
              " 'pyramids': 247,\n",
              " 'giza': 248,\n",
              " 'egypt': 249,\n",
              " 'sea': 250,\n",
              " 'creature': 251,\n",
              " 'eight': 252,\n",
              " 'arms': 253,\n",
              " 'octopus': 254,\n",
              " 'holiday': 255,\n",
              " 'celebrated': 256,\n",
              " 'december': 257,\n",
              " '25': 258,\n",
              " 'christmas': 259,\n",
              " 'yen': 260,\n",
              " 'legs': 261,\n",
              " 'spider': 262,\n",
              " 'sport': 263,\n",
              " 'uses': 264,\n",
              " 'net,': 265,\n",
              " 'ball,': 266,\n",
              " 'hoop': 267,\n",
              " 'basketball': 268,\n",
              " 'kangaroos': 269,\n",
              " 'female': 270,\n",
              " 'minister': 271,\n",
              " 'uk': 272,\n",
              " 'margaretthatcher': 273,\n",
              " 'fastest': 274,\n",
              " 'animal': 275,\n",
              " 'cheetah': 276,\n",
              " 'periodic': 277,\n",
              " 'table': 278,\n",
              " 'spain': 279,\n",
              " 'madrid': 280,\n",
              " 'closest': 281,\n",
              " 'sun': 282,\n",
              " 'father': 283,\n",
              " 'computers': 284,\n",
              " 'charlesbabbage': 285,\n",
              " 'mexico': 286,\n",
              " 'mexicocity': 287,\n",
              " 'colors': 288,\n",
              " 'rainbow': 289,\n",
              " 'musical': 290,\n",
              " 'instrument': 291,\n",
              " 'black': 292,\n",
              " 'white': 293,\n",
              " 'keys': 294,\n",
              " 'piano': 295,\n",
              " 'americas': 296,\n",
              " '1492': 297,\n",
              " 'christophercolumbus': 298,\n",
              " 'disney': 299,\n",
              " 'character': 300,\n",
              " 'long': 301,\n",
              " 'nose': 302,\n",
              " 'grows': 303,\n",
              " 'it': 304,\n",
              " 'when': 305,\n",
              " 'lying': 306,\n",
              " 'pinocchio': 307,\n",
              " 'directed': 308,\n",
              " 'movie': 309,\n",
              " 'titanic': 310,\n",
              " 'jamescameron': 311,\n",
              " 'superhero': 312,\n",
              " 'also': 313,\n",
              " 'dark': 314,\n",
              " 'knight': 315,\n",
              " 'batman': 316,\n",
              " 'brasilia': 317,\n",
              " 'fruit': 318,\n",
              " 'king': 319,\n",
              " 'fruits': 320,\n",
              " 'mango': 321,\n",
              " 'eiffel': 322,\n",
              " 'tower': 323}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self , df , vocab):\n",
        "    self.df = df\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    numerical_question = text_to_indices(self.df.iloc[index]['question'] , self.vocab)\n",
        "    numerical_answers  = text_to_indices(self.df.iloc[index]['answer']   , self.vocab)\n",
        "\n",
        "    return torch.tensor(numerical_question) ,torch.tensor(numerical_answers)"
      ],
      "metadata": {
        "id": "OIXS781jyNNi"
      },
      "id": "OIXS781jyNNi",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = QADataset(df, vocab)"
      ],
      "metadata": {
        "id": "y23-JemV28JQ"
      },
      "id": "y23-JemV28JQ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1 , shuffle=True)"
      ],
      "metadata": {
        "id": "P3G7BIBH3asK"
      },
      "id": "P3G7BIBH3asK",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in dataloader:\n",
        "  print(question , answer)"
      ],
      "metadata": {
        "id": "hK-PvHrS3mVi",
        "outputId": "4a2e3391-fad7-49bd-bf49-6a85e9bb5228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "hK-PvHrS3mVi",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
            "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
            "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
            "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
            "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
            "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
            "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
            "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
            "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
            "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
            "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
            "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
            "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
            "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
            "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
            "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
            "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
            "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
            "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
            "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
            "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
            "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
            "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
            "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
            "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
            "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
            "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
            "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
            "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
            "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
            "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
            "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
            "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
            "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
            "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
            "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
            "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
            "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
            "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
            "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
            "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
            "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
            "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
            "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
            "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
            "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
            "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
            "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
            "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
            "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
            "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
            "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
            "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
            "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
            "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
            "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
            "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
            "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
            "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
            "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
            "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
            "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
            "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
            "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
            "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
            "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
            "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
            "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
            "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
            "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
            "tensor([[10, 75, 76]]) tensor([[77]])\n",
            "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
            "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
            "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
            "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
            "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
            "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
            "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
            "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
            "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
            "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
            "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
            "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
            "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple rnn architecture\n",
        "class simpleRNN(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
        "    self.rnn = nn.RNN(50, 64, batch_first=True) # Added batch_first=True\n",
        "    self.fc = nn.Linear(64, vocab_size)\n",
        "\n",
        "  def forward(self, question):\n",
        "    embedded_question = self.embedding(question)\n",
        "    output, hidden = self.rnn(embedded_question)\n",
        "    return self.fc(output[:, -1, :])"
      ],
      "metadata": {
        "id": "O4kuVe3s3rmW"
      },
      "id": "O4kuVe3s3rmW",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstration of the first layer , ie embedding layer\n",
        "print(dataset[0])\n",
        "print(dataset[0][0])\n",
        "print(dataset[0][1])\n",
        "\n",
        "x = nn.Embedding(324, embedding_dim = 50)\n",
        "a = x(dataset[0][0])\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-fVgicLWa8FQ",
        "outputId": "ea641323-6390-463d-e2f4-d11b32a5ba4c"
      },
      "id": "-fVgicLWa8FQ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([1, 2, 3, 4, 5, 6]), tensor([7]))\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([7])\n",
            "tensor([[ 1.9510,  0.3835, -0.6742,  1.0170,  0.3757, -0.0587,  0.7233, -0.0578,\n",
            "          0.4467, -0.4602,  1.4304, -1.1047,  0.7228,  0.0426, -0.8533,  0.0822,\n",
            "          1.8872, -0.3111, -0.1372, -0.9485, -1.2271, -1.8420, -1.4401,  0.2846,\n",
            "         -1.3248,  1.9686, -0.7289,  1.3958,  0.0195,  0.8718, -1.6931,  0.0696,\n",
            "          1.6488, -0.1942,  0.9614, -0.4591,  1.3682, -0.8834,  0.9048,  1.0803,\n",
            "         -0.8558,  0.5434,  0.7124,  0.6957,  0.5596, -0.2251, -1.4040, -0.8118,\n",
            "          1.1291,  1.3819],\n",
            "        [ 1.2057,  0.7242,  0.4766,  1.8232, -0.1568, -0.6864, -0.3358,  1.4507,\n",
            "         -0.2730, -0.7884, -0.2710, -0.6521, -0.9552,  0.4701,  1.9696, -0.0051,\n",
            "          0.9150,  0.7413,  0.9815,  0.7858, -0.8250,  1.1154, -0.1230,  0.2708,\n",
            "         -0.0479, -1.0042, -1.3345,  2.1161, -0.1875,  0.1872, -0.4203, -0.7981,\n",
            "         -1.0904,  0.0945, -0.0390,  0.5773, -0.1296,  1.0336, -1.3871, -1.2780,\n",
            "         -2.1076, -1.6968, -0.7763,  1.7590, -0.8475, -0.1707, -0.4166,  1.1461,\n",
            "          1.2176,  0.1312],\n",
            "        [-0.1650, -0.5918, -0.5887, -0.4289,  1.4803,  0.8817,  1.5964,  0.2634,\n",
            "          0.2195, -0.8455, -1.5055,  0.9468,  0.9697, -0.6787,  0.8267,  1.1378,\n",
            "          0.8813,  0.1568, -1.4486, -1.3714, -0.6260,  0.2535, -0.7751, -0.0134,\n",
            "          1.4369, -1.8550, -1.0911,  1.1103, -0.1388,  0.2915, -1.1944,  0.7628,\n",
            "          0.8531, -0.7405,  0.5946,  0.4819, -1.4626,  0.1602,  0.0688, -0.5608,\n",
            "         -1.8279,  0.4371, -1.2807, -0.8067,  0.6496,  0.4748,  0.8091,  0.8877,\n",
            "         -1.0736,  0.7802],\n",
            "        [ 0.1613, -2.1111, -0.7972, -1.1730, -1.8348,  0.2034,  0.0593, -1.0446,\n",
            "          1.6875, -0.9481, -0.3383,  1.2903, -0.2741,  0.6321, -0.5650,  0.1769,\n",
            "          0.6204,  0.1846,  1.6184, -1.3665,  1.0243,  1.4934,  2.3173, -0.2716,\n",
            "          0.0120, -0.6922,  0.5497, -1.0852, -2.1457,  0.6080, -0.2618,  0.1508,\n",
            "         -1.8769,  0.5295, -0.1151,  1.0186, -1.6859,  1.2083, -0.0695, -0.6478,\n",
            "          1.3092, -2.0131, -0.1831, -0.4487, -1.5585, -0.0433,  1.1896,  1.2030,\n",
            "         -0.2177, -1.3959],\n",
            "        [ 0.7647, -0.2993, -0.5648, -0.2376,  0.6428, -0.6393,  1.5892,  0.8782,\n",
            "         -1.5814,  0.1801, -1.8107, -0.7149, -0.7086,  0.4580, -0.1945,  1.1173,\n",
            "         -0.4875,  0.8200, -1.7597,  0.2933,  0.0111,  0.3567,  0.7608, -0.2147,\n",
            "          0.3806,  0.6806,  1.5111, -0.6460, -1.0508,  0.9376, -0.4013,  0.6174,\n",
            "          0.1434, -0.1582, -0.5082, -0.3067,  1.8152,  0.6498,  0.8128, -0.4257,\n",
            "          1.3336, -0.1489,  0.9421, -0.3895, -0.3910,  1.9154,  0.9952, -0.1104,\n",
            "          0.7836, -1.9680],\n",
            "        [-1.0081,  1.6373,  0.0057,  0.3578,  0.4065,  1.3347, -0.1972, -0.7052,\n",
            "          2.2618, -1.0063, -1.5496,  0.0205, -0.4420, -1.2132, -0.4138,  1.0680,\n",
            "         -0.1454,  1.8660,  2.2518, -0.2150,  0.9740, -2.1942, -0.3491,  0.3693,\n",
            "          1.9886,  0.3473, -1.5664,  1.1044, -1.7989,  2.0884,  0.2727,  0.9943,\n",
            "          1.1399,  1.0571, -1.7229, -0.3284,  0.4280, -1.1765, -0.8856, -0.1965,\n",
            "          0.1654, -0.4643,  0.7193,  0.4568, -1.8437, -0.7557, -0.4042, -0.3476,\n",
            "          1.5703, -0.1752]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstration of the second layer ,ie rnn layer\n",
        "y = nn.RNN(50 ,64)\n",
        "# print(y)\n",
        "print(y(a))\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(y(a)[0]) #thsi prints the shape of the output tensor\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(y(a)[1]) #this prints the shape of the hidden tensor\n",
        "\n",
        "b = y(a)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O3-8qXVbX8E",
        "outputId": "670ea65f-bad6-410c-d238-edb6b9241080",
        "collapsed": true
      },
      "id": "5O3-8qXVbX8E",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-0.6204, -0.0203, -0.2887,  0.1660,  0.1322,  0.3559, -0.8063,  0.3843,\n",
            "         -0.5506, -0.5945, -0.2307,  0.6002,  0.9458, -0.1357,  0.8379, -0.3584,\n",
            "         -0.4037, -0.1949, -0.5161,  0.0563, -0.1205,  0.1823,  0.4058,  0.5339,\n",
            "         -0.6748,  0.4014, -0.3009, -0.2334, -0.1077, -0.3784, -0.2661, -0.1139,\n",
            "          0.4789, -0.1183, -0.6610,  0.2185, -0.6982, -0.1785,  0.7554,  0.2535,\n",
            "         -0.3437, -0.5492, -0.4923,  0.6360,  0.2342,  0.4381,  0.3909, -0.5424,\n",
            "         -0.2362, -0.0358,  0.1849, -0.1653,  0.1854, -0.0224,  0.6396, -0.1813,\n",
            "          0.2941,  0.2918, -0.6228,  0.2380, -0.0316,  0.0291, -0.3072, -0.6183],\n",
            "        [ 0.7398, -0.0709,  0.0421, -0.4208,  0.3960, -0.0672, -0.5374, -0.3949,\n",
            "          0.6273,  0.3572,  0.2788,  0.1407,  0.3216, -0.0853,  0.3703, -0.0172,\n",
            "          0.7241,  0.6509, -0.3759,  0.2308, -0.6632,  0.6344, -0.0020, -0.3236,\n",
            "          0.5333,  0.0982,  0.2089,  0.7473, -0.3545, -0.2656, -0.0194,  0.1881,\n",
            "          0.2953, -0.3096,  0.2060, -0.2306,  0.1491,  0.3048, -0.4394, -0.4041,\n",
            "         -0.5715, -0.1596,  0.9618,  0.2192, -0.6407, -0.5476, -0.1153, -0.1866,\n",
            "         -0.1644, -0.4833,  0.2124, -0.0105,  0.0965,  0.6017, -0.1851,  0.1279,\n",
            "         -0.3770, -0.2468,  0.3317, -0.5243, -0.0142, -0.3032, -0.3840, -0.3366],\n",
            "        [ 0.4944, -0.5242, -0.3518, -0.7654,  0.7854, -0.1494,  0.0455, -0.3008,\n",
            "          0.3147,  0.1106, -0.5166, -0.5812,  0.2497, -0.4155, -0.7003, -0.5686,\n",
            "          0.7829, -0.3425,  0.2155, -0.2036, -0.0634, -0.8047, -0.4831,  0.3968,\n",
            "         -0.4292, -0.5014,  0.5002,  0.8491, -0.6717,  0.0230,  0.6340,  0.0631,\n",
            "          0.4107,  0.2291,  0.2325,  0.7147, -0.8973, -0.0637,  0.6171,  0.6901,\n",
            "         -0.2594,  0.2822,  0.0729,  0.5605,  0.2472, -0.4481,  0.2132,  0.1708,\n",
            "         -0.1228,  0.0831,  0.1459, -0.3611, -0.3600, -0.2064,  0.1427,  0.2084,\n",
            "          0.4140, -0.6720,  0.2489, -0.6793,  0.5685, -0.0263, -0.1856, -0.4431],\n",
            "        [ 0.8462, -0.5071, -0.5052, -0.4299, -0.2466,  0.3474,  0.1850, -0.1423,\n",
            "          0.7859, -0.4311,  0.0549, -0.8498, -0.8485, -0.2419, -0.0551,  0.1392,\n",
            "          0.7707,  0.4496, -0.2597,  0.5276,  0.5723, -0.6001, -0.6536, -0.2608,\n",
            "          0.2786, -0.4417,  0.9207,  0.6234,  0.2360,  0.1857,  0.6834,  0.4913,\n",
            "         -0.1567,  0.3975,  0.2157, -0.1565,  0.5384, -0.4949, -0.1770,  0.3748,\n",
            "         -0.1171, -0.1980,  0.7949,  0.2923, -0.4057, -0.2767, -0.6370,  0.4737,\n",
            "          0.3118, -0.1482,  0.0637,  0.7518, -0.8328, -0.7791,  0.5113, -0.1500,\n",
            "          0.2027, -0.2743, -0.6719,  0.7135, -0.9137,  0.1210, -0.5794, -0.6182],\n",
            "        [-0.3102, -0.6670,  0.4866,  0.0355,  0.8618,  0.5018,  0.4177, -0.0736,\n",
            "          0.3159,  0.4116,  0.5385, -0.3220, -0.6125,  0.4355,  0.0156,  0.0118,\n",
            "          0.6050,  0.4200, -0.1711, -0.7086,  0.2287, -0.2245, -0.5575, -0.4255,\n",
            "          0.4563, -0.5185, -0.3450,  0.5097, -0.0090, -0.0243,  0.4423, -0.2178,\n",
            "          0.7543, -0.2292,  0.4061,  0.5978, -0.0876, -0.0503,  0.2294, -0.2590,\n",
            "         -0.2591,  0.1241, -0.4896, -0.7148,  0.4400,  0.4997, -0.3375, -0.0197,\n",
            "          0.1988,  0.2168, -0.3839,  0.6136,  0.2263, -0.5731, -0.1306,  0.6445,\n",
            "         -0.2887, -0.3397, -0.0561, -0.0141, -0.0527,  0.5358,  0.6655, -0.4990],\n",
            "        [ 0.1905, -0.3300,  0.6606, -0.7799,  0.1543,  0.6596, -0.0123, -0.2388,\n",
            "         -0.7893, -0.7615,  0.7565,  0.0748,  0.2031, -0.4171,  0.5075, -0.2353,\n",
            "          0.4211,  0.6828, -0.5902,  0.4907,  0.1898, -0.4222, -0.4487,  0.1231,\n",
            "          0.3274,  0.2008, -0.3953,  0.6804,  0.0790,  0.1486, -0.1207,  0.7952,\n",
            "         -0.7401, -0.4248, -0.6251,  0.5393, -0.6315, -0.6405,  0.0801,  0.6088,\n",
            "         -0.1876,  0.4299, -0.0906,  0.8154,  0.6963, -0.1061,  0.0228,  0.2708,\n",
            "          0.6527,  0.2176,  0.4127, -0.1301, -0.2345, -0.1524,  0.1078, -0.6144,\n",
            "         -0.0537,  0.2724, -0.6434,  0.4503,  0.0084,  0.0161,  0.3162, -0.7276]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 0.1905, -0.3300,  0.6606, -0.7799,  0.1543,  0.6596, -0.0123, -0.2388,\n",
            "         -0.7893, -0.7615,  0.7565,  0.0748,  0.2031, -0.4171,  0.5075, -0.2353,\n",
            "          0.4211,  0.6828, -0.5902,  0.4907,  0.1898, -0.4222, -0.4487,  0.1231,\n",
            "          0.3274,  0.2008, -0.3953,  0.6804,  0.0790,  0.1486, -0.1207,  0.7952,\n",
            "         -0.7401, -0.4248, -0.6251,  0.5393, -0.6315, -0.6405,  0.0801,  0.6088,\n",
            "         -0.1876,  0.4299, -0.0906,  0.8154,  0.6963, -0.1061,  0.0228,  0.2708,\n",
            "          0.6527,  0.2176,  0.4127, -0.1301, -0.2345, -0.1524,  0.1078, -0.6144,\n",
            "         -0.0537,  0.2724, -0.6434,  0.4503,  0.0084,  0.0161,  0.3162, -0.7276]],\n",
            "       grad_fn=<SqueezeBackward1>))\n",
            "-----------------------------------------------------------------------\n",
            "tensor([[-0.6204, -0.0203, -0.2887,  0.1660,  0.1322,  0.3559, -0.8063,  0.3843,\n",
            "         -0.5506, -0.5945, -0.2307,  0.6002,  0.9458, -0.1357,  0.8379, -0.3584,\n",
            "         -0.4037, -0.1949, -0.5161,  0.0563, -0.1205,  0.1823,  0.4058,  0.5339,\n",
            "         -0.6748,  0.4014, -0.3009, -0.2334, -0.1077, -0.3784, -0.2661, -0.1139,\n",
            "          0.4789, -0.1183, -0.6610,  0.2185, -0.6982, -0.1785,  0.7554,  0.2535,\n",
            "         -0.3437, -0.5492, -0.4923,  0.6360,  0.2342,  0.4381,  0.3909, -0.5424,\n",
            "         -0.2362, -0.0358,  0.1849, -0.1653,  0.1854, -0.0224,  0.6396, -0.1813,\n",
            "          0.2941,  0.2918, -0.6228,  0.2380, -0.0316,  0.0291, -0.3072, -0.6183],\n",
            "        [ 0.7398, -0.0709,  0.0421, -0.4208,  0.3960, -0.0672, -0.5374, -0.3949,\n",
            "          0.6273,  0.3572,  0.2788,  0.1407,  0.3216, -0.0853,  0.3703, -0.0172,\n",
            "          0.7241,  0.6509, -0.3759,  0.2308, -0.6632,  0.6344, -0.0020, -0.3236,\n",
            "          0.5333,  0.0982,  0.2089,  0.7473, -0.3545, -0.2656, -0.0194,  0.1881,\n",
            "          0.2953, -0.3096,  0.2060, -0.2306,  0.1491,  0.3048, -0.4394, -0.4041,\n",
            "         -0.5715, -0.1596,  0.9618,  0.2192, -0.6407, -0.5476, -0.1153, -0.1866,\n",
            "         -0.1644, -0.4833,  0.2124, -0.0105,  0.0965,  0.6017, -0.1851,  0.1279,\n",
            "         -0.3770, -0.2468,  0.3317, -0.5243, -0.0142, -0.3032, -0.3840, -0.3366],\n",
            "        [ 0.4944, -0.5242, -0.3518, -0.7654,  0.7854, -0.1494,  0.0455, -0.3008,\n",
            "          0.3147,  0.1106, -0.5166, -0.5812,  0.2497, -0.4155, -0.7003, -0.5686,\n",
            "          0.7829, -0.3425,  0.2155, -0.2036, -0.0634, -0.8047, -0.4831,  0.3968,\n",
            "         -0.4292, -0.5014,  0.5002,  0.8491, -0.6717,  0.0230,  0.6340,  0.0631,\n",
            "          0.4107,  0.2291,  0.2325,  0.7147, -0.8973, -0.0637,  0.6171,  0.6901,\n",
            "         -0.2594,  0.2822,  0.0729,  0.5605,  0.2472, -0.4481,  0.2132,  0.1708,\n",
            "         -0.1228,  0.0831,  0.1459, -0.3611, -0.3600, -0.2064,  0.1427,  0.2084,\n",
            "          0.4140, -0.6720,  0.2489, -0.6793,  0.5685, -0.0263, -0.1856, -0.4431],\n",
            "        [ 0.8462, -0.5071, -0.5052, -0.4299, -0.2466,  0.3474,  0.1850, -0.1423,\n",
            "          0.7859, -0.4311,  0.0549, -0.8498, -0.8485, -0.2419, -0.0551,  0.1392,\n",
            "          0.7707,  0.4496, -0.2597,  0.5276,  0.5723, -0.6001, -0.6536, -0.2608,\n",
            "          0.2786, -0.4417,  0.9207,  0.6234,  0.2360,  0.1857,  0.6834,  0.4913,\n",
            "         -0.1567,  0.3975,  0.2157, -0.1565,  0.5384, -0.4949, -0.1770,  0.3748,\n",
            "         -0.1171, -0.1980,  0.7949,  0.2923, -0.4057, -0.2767, -0.6370,  0.4737,\n",
            "          0.3118, -0.1482,  0.0637,  0.7518, -0.8328, -0.7791,  0.5113, -0.1500,\n",
            "          0.2027, -0.2743, -0.6719,  0.7135, -0.9137,  0.1210, -0.5794, -0.6182],\n",
            "        [-0.3102, -0.6670,  0.4866,  0.0355,  0.8618,  0.5018,  0.4177, -0.0736,\n",
            "          0.3159,  0.4116,  0.5385, -0.3220, -0.6125,  0.4355,  0.0156,  0.0118,\n",
            "          0.6050,  0.4200, -0.1711, -0.7086,  0.2287, -0.2245, -0.5575, -0.4255,\n",
            "          0.4563, -0.5185, -0.3450,  0.5097, -0.0090, -0.0243,  0.4423, -0.2178,\n",
            "          0.7543, -0.2292,  0.4061,  0.5978, -0.0876, -0.0503,  0.2294, -0.2590,\n",
            "         -0.2591,  0.1241, -0.4896, -0.7148,  0.4400,  0.4997, -0.3375, -0.0197,\n",
            "          0.1988,  0.2168, -0.3839,  0.6136,  0.2263, -0.5731, -0.1306,  0.6445,\n",
            "         -0.2887, -0.3397, -0.0561, -0.0141, -0.0527,  0.5358,  0.6655, -0.4990],\n",
            "        [ 0.1905, -0.3300,  0.6606, -0.7799,  0.1543,  0.6596, -0.0123, -0.2388,\n",
            "         -0.7893, -0.7615,  0.7565,  0.0748,  0.2031, -0.4171,  0.5075, -0.2353,\n",
            "          0.4211,  0.6828, -0.5902,  0.4907,  0.1898, -0.4222, -0.4487,  0.1231,\n",
            "          0.3274,  0.2008, -0.3953,  0.6804,  0.0790,  0.1486, -0.1207,  0.7952,\n",
            "         -0.7401, -0.4248, -0.6251,  0.5393, -0.6315, -0.6405,  0.0801,  0.6088,\n",
            "         -0.1876,  0.4299, -0.0906,  0.8154,  0.6963, -0.1061,  0.0228,  0.2708,\n",
            "          0.6527,  0.2176,  0.4127, -0.1301, -0.2345, -0.1524,  0.1078, -0.6144,\n",
            "         -0.0537,  0.2724, -0.6434,  0.4503,  0.0084,  0.0161,  0.3162, -0.7276]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "-----------------------------------------------------------------------\n",
            "tensor([[ 0.1905, -0.3300,  0.6606, -0.7799,  0.1543,  0.6596, -0.0123, -0.2388,\n",
            "         -0.7893, -0.7615,  0.7565,  0.0748,  0.2031, -0.4171,  0.5075, -0.2353,\n",
            "          0.4211,  0.6828, -0.5902,  0.4907,  0.1898, -0.4222, -0.4487,  0.1231,\n",
            "          0.3274,  0.2008, -0.3953,  0.6804,  0.0790,  0.1486, -0.1207,  0.7952,\n",
            "         -0.7401, -0.4248, -0.6251,  0.5393, -0.6315, -0.6405,  0.0801,  0.6088,\n",
            "         -0.1876,  0.4299, -0.0906,  0.8154,  0.6963, -0.1061,  0.0228,  0.2708,\n",
            "          0.6527,  0.2176,  0.4127, -0.1301, -0.2345, -0.1524,  0.1078, -0.6144,\n",
            "         -0.0537,  0.2724, -0.6434,  0.4503,  0.0084,  0.0161,  0.3162, -0.7276]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstration of the third layer, ie, the linear layer\n",
        "z = nn.Linear(64, 324)\n",
        "z(b).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jk2B2N4dNub",
        "outputId": "be45654e-9cc2-4173-ac40-9085f011f9fd"
      },
      "id": "-jk2B2N4dNub",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 324])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is being useful in case of debugging the code, when the modle fails\n",
        "x = nn.Embedding(324, embedding_dim=50)\n",
        "y = nn.RNN(50, 64, batch_first=True)\n",
        "z = nn.Linear(64, 324)\n",
        "\n",
        "a = dataset[0][0].reshape(1,6)\n",
        "print(\"shape of a:\", a.shape)\n",
        "b = x(a)\n",
        "print(\"shape of b:\", b.shape)\n",
        "c, d = y(b)\n",
        "print(\"shape of c:\", c.shape)\n",
        "print(\"shape of d:\", d.shape)\n",
        "\n",
        "e = z(d.squeeze(0))\n",
        "\n",
        "print(\"shape of e:\", e.shape)"
      ],
      "metadata": {
        "id": "8byrTnwElLH1",
        "outputId": "a463bce6-871d-4366-8da7-3f4d086d781a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8byrTnwElLH1",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of a: torch.Size([1, 6])\n",
            "shape of b: torch.Size([1, 6, 50])\n",
            "shape of c: torch.Size([1, 6, 64])\n",
            "shape of d: torch.Size([1, 1, 64])\n",
            "shape of e: torch.Size([1, 324])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL initialization\n",
        "learning_rate = 0.001\n",
        "epochs = 30\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = simpleRNN(len(vocab))\n",
        "optimizer = optim.Adam(model.parameters() , lr =learning_rate)"
      ],
      "metadata": {
        "id": "lNuaSokifWDi"
      },
      "id": "lNuaSokifWDi",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "  total_loss =0\n",
        "\n",
        "  for question , answer in dataloader:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(question)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(output , answer.squeeze(1)) # Corrected target shape by squeezing\n",
        "\n",
        "    # gradients\n",
        "    loss.backward() # Corrected from model.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "  print(f\"epochs{epoch+1} , loss{total_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKZDWaKFhH9p",
        "outputId": "f5508d4a-fc89-4010-ec13-c085ed4d48bf"
      },
      "id": "lKZDWaKFhH9p",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs1 , loss526.6085948944092\n",
            "epochs2 , loss461.8344326019287\n",
            "epochs3 , loss385.1834497451782\n",
            "epochs4 , loss318.1324818134308\n",
            "epochs5 , loss265.6491975784302\n",
            "epochs6 , loss217.5273550748825\n",
            "epochs7 , loss173.4023202061653\n",
            "epochs8 , loss135.26015710830688\n",
            "epochs9 , loss104.60088697075844\n",
            "epochs10 , loss79.9649957716465\n",
            "epochs11 , loss60.964148074388504\n",
            "epochs12 , loss47.721647784113884\n",
            "epochs13 , loss37.636601984500885\n",
            "epochs14 , loss30.14390578120947\n",
            "epochs15 , loss24.787516459822655\n",
            "epochs16 , loss20.47341265529394\n",
            "epochs17 , loss17.176109820604324\n",
            "epochs18 , loss14.542703792452812\n",
            "epochs19 , loss12.534499067813158\n",
            "epochs20 , loss10.898029550909996\n",
            "epochs21 , loss9.512839451432228\n",
            "epochs22 , loss8.454972725361586\n",
            "epochs23 , loss7.474194344133139\n",
            "epochs24 , loss6.669783689081669\n",
            "epochs25 , loss6.035321770235896\n",
            "epochs26 , loss5.436522055417299\n",
            "epochs27 , loss4.934913970530033\n",
            "epochs28 , loss4.506783422082663\n",
            "epochs29 , loss4.121497560292482\n",
            "epochs30 , loss3.7738420516252518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, question, threshold=0.5):\n",
        "\n",
        "  # convert question to numbers\n",
        "  numerical_question = text_to_indices(question, vocab)\n",
        "\n",
        "  # tensor\n",
        "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
        "\n",
        "  # send to model\n",
        "  output = model(question_tensor)\n",
        "\n",
        "  # convert logits to probs\n",
        "  probs = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "  # find index of max prob\n",
        "  value, index = torch.max(probs, dim=1)\n",
        "\n",
        "  if value < threshold:\n",
        "    print(\"I don't know\")\n",
        "\n",
        "  print(list(vocab.keys())[index])"
      ],
      "metadata": {
        "id": "iLwVMRifj2RH"
      },
      "id": "iLwVMRifj2RH",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, \"What is the largest in our solar system?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SUZZ9PPkWyD",
        "outputId": "5dc37d95-a510-4d3f-fca7-d859bf70d723"
      },
      "id": "-SUZZ9PPkWyD",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jupiter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(vocab.keys())[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kIqYr9gZkXm8",
        "outputId": "1920858e-f7dc-4b3c-d721-dd1897db73f1"
      },
      "id": "kIqYr9gZkXm8",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'germany'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "name": "pytorch_10_rnn_qa_system.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}